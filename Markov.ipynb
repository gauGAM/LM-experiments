{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Markov.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZUl45/iWufqB4tX/VTvHU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vnc6Kf4zcT7O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "def word_tokenizer(input : str, n_unique : int=None):\n",
        "  '''\n",
        "  Converts input string into an array of word tokens, splitting at any whitespace.\n",
        "  Parameters:\n",
        "    input (str): input with whitespace\n",
        "    n_unique (int): number of unique tokens. The most common n-1 tokens in the original vocabulary are kept;\n",
        "    all tokens outside this vocabulary are cast to <unk>\n",
        "  Returns:\n",
        "    tokens (np.ndarray of str)\n",
        "  '''\n",
        "  tokens = np.array(input.split())\n",
        "  unique_tokens = np.unique(tokens, return_counts=True)\n",
        "  if n_unique is not None:\n",
        "    unique_tokens_freq_sort = unique_tokens[0][np.argsort(-unique_tokens[1])]\n",
        "    unk_tokens = unique_tokens_freq_sort[n_unique-1:len(unique_tokens_freq_sort)]\n",
        "    tokens[np.isin(tokens, unk_tokens)] = '<unk>'\n",
        "  return np.array(tokens)\n",
        "class MarkovChain:\n",
        "  '''\n",
        "  MarkovChain - can be trained to get transition probabilities, can be evaluated to get next n words.\n",
        "  The format is kind of silly for markov chains, but should be something which can be\n",
        "  relatively unmodified as the model complexity and training sample size grow\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    return\n",
        "  def train(self, X : np.ndarray, pseudocount : int=0):\n",
        "    '''\n",
        "    Trains markov chain (populates transition probability matrix)\n",
        "    Parameters:\n",
        "      X (np.ndarray): tokens. This should already be of a reasonable vocabulary size \n",
        "        (i.e. you should replace extra words with <unk> in the tokenizer!)\n",
        "      pseudocount (int): amount to add to each element of count matrix (as a form of regularization) \n",
        "    '''\n",
        "    unique = np.unique(X)\n",
        "    # I honestly am not sure if there's a way to do this which relies more on built-in numpy functions\n",
        "    # but its big-O runtime is constrained by O(v^2 + n) anyways, which I don't think we can improve upon.\n",
        "    token_to_idx = dict()\n",
        "    for i in range(len(unique)):\n",
        "      word = unique[i]\n",
        "      token_to_idx[word] = i\n",
        "    # we could use a sparse matrix here but that only works if we have 0 for pseudocount; probably fine either way.\n",
        "    # It's a toy model!\n",
        "    transition_matrix = np.zeros([len(unique), len(unique)])\n",
        "    prev_token = None\n",
        "    for token in X:\n",
        "      if prev_token is not None:\n",
        "        prev_idx = token_to_idx[prev_token]\n",
        "        cur_idx = token_to_idx[token]\n",
        "        transition_matrix[prev_idx, cur_idx] += 1\n",
        "      prev_token = token\n",
        "    for i in range(len(unique)):\n",
        "      transition_matrix[i] /= np.sum(transition_matrix[i])\n",
        "    self.transition_matrix = transition_matrix\n",
        "    self.token_to_idx = token_to_idx\n",
        "  def predict(self, X : np.ndarray, n : int=1):\n",
        "    '''\n",
        "    Continues input token array for next n tokens (cannot specify empty X)\n",
        "    Parameters:\n",
        "      X (np.ndarray): tokens to continue\n",
        "      n (int): number of tokens to output\n",
        "    Returns:\n",
        "      continuation (np.ndarray): tokens to output\n",
        "    '''\n",
        "    idx_to_token = {idx: token for token, idx in self.token_to_idx.items()}\n",
        "    token_to_idx = self.token_to_idx\n",
        "    prev_idx = token_to_idx[X[-1]]\n",
        "    res_tokens = []\n",
        "    for i in range(n):\n",
        "      transition_probs = self.transition_matrix[prev_idx]\n",
        "      next_idx = np.random.choice(len(token_to_idx), size=1, p=transition_probs)[0]\n",
        "      res_tokens.append(idx_to_token[next_idx])\n",
        "      prev_idx = next_idx\n",
        "    return np.array(res_tokens)\n",
        "  # TODO maybe worth adding a get_log_likelihood thing here to compare loss with other models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testStr = 'a a a b b b a a b b b b b b a a a a b b b c c a c c c a b b b c a a a c a a a a a a a a a a a'\n",
        "tokens = word_tokenizer(testStr)\n",
        "model = MarkovChain()\n",
        "model.train(tokens)\n",
        "model.predict(tokens, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0BQDvbvdNbj",
        "outputId": "fd714ff1-4c7a-48ca-c31d-1cc90bbfdf47"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'c', 'c'], dtype='<U1')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "  \n",
        "# downloaded from https://deepai.org/dataset/text8\n",
        "# upon the recommendation of a reddit post saying it's a good dataset with a small vocab size\n",
        "# which is good for model testing while we're still using word-level tokenization\n",
        "file_name = \"text8.zip\"\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  text8_raw = str(zip.read('text8'))"
      ],
      "metadata": {
        "id": "2RiMpGVCDN9T"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text8_raw[0:1000])\n",
        "# 4MB, seems reasonable\n",
        "text8_raw_small = text8_raw[0:4000000]\n",
        "tokens = word_tokenizer(text8_raw_small, n_unique=5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVfGZCfREAnM",
        "outputId": "ce3a8244-e05d-4140-c690-c284b5b37070"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b' anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the state the word anarchy as most anarchists use it does not imply chaos nihilism or anomie but rather a harmonious anti authoritarian society in place of what are regarded as authoritarian political structures and coercive economic institu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tokens = np.unique(tokens, return_counts=True)\n",
        "unique_tokens_freq_sort = unique_tokens[0][np.argsort(-unique_tokens[1])]\n",
        "print(unique_tokens_freq_sort)\n",
        "unique_tokens_counts = unique_tokens[1][np.argsort(-unique_tokens[1])]\n",
        "print(unique_tokens_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNjH_gJYEqwO",
        "outputId": "f2192311-4ad4-4753-e9e6-d97cb6d192ad"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<unk>' 'the' 'of' ... 'mystic' 'celestial' 'sole']\n",
            "[91882 42015 24097 ...    13    13    13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlucky. Even with 5000 unique words, there are twice as many unknown words as 'the's, and we are beginning to hit the limits of what is feasible under this implementation in terms of memory usage."
      ],
      "metadata": {
        "id": "IWDTeMSkNFHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MarkovChain()\n",
        "model.train(tokens)\n",
        "# apparently np.random.seed is bad for use in specific classes,\n",
        "# but it's probably fine for a notebook setting.\n",
        "np.random.seed(1234)\n",
        "model.predict(np.array(['the']), 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X5WCGifNQi8",
        "outputId": "4675186a-05a1-4628-865c-6592ccc2bc53"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['<unk>', 'might', 'be', 'seen', 'much', 'improved', 'but', 'the',\n",
              "       'village', 'see', 'also', 'has', 'low', 'point', 'in',\n",
              "       'particular', 'individuals', '<unk>', 'rival', 'newspapers', 'are',\n",
              "       'normally', '<unk>', 'by', 'their', 'name', 'for', 'the',\n",
              "       'central', 'district', 'the', 'faithful', 'saints', '<unk>', 'one',\n",
              "       'one', 'four', 'zero', 's', 'there', 'are', 'against', '<unk>',\n",
              "       'of', 'property', 'is', '<unk>', 'inspired', 'by', 'jews', 'and',\n",
              "       'moderate', 'republican', '<unk>', 'may', 'therefore', 'stephen',\n",
              "       'v', 'v', 'pp', 'four', 'seven', 'one', 'five', 'little', '<unk>',\n",
              "       'football', 'while', '<unk>', '<unk>', 'parts', 'of', 'it', 'as',\n",
              "       'a', 'tax', 'bases', 'its', '<unk>', 'animation', 'film', 'making',\n",
              "       'them', 'other', 'pop', 'art', 'opened', 'the', 'murder', 'is',\n",
              "       'a', 'means', 'of', 'time', 'largely', 'composed', 'of', 'the',\n",
              "       '<unk>', 'of'], dtype='<U11')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oof. At the very least, we see some nice two-word patterns, and even three-word patterns as a result of chained two-word associations. For example, we see 'might be' and 'be seen' chained into 'might be seen', which is a coherent three-word phrase! Still, it seems to have no awareness of where it is in a sentence. To be honest, more sentence-level awareness might be a bit difficult to obtain with the text8 dataset, even with a more advanced model, as it does not have punctuation. It's probably better to apply methods like byte-pair encoding before incorporating more complex data so as to avoid wasting time hard-coding a bunch of data cleaning heuristics.\n"
      ],
      "metadata": {
        "id": "qU1ObWJdNlek"
      }
    }
  ]
}