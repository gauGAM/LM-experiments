{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Markov.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4O9IsjIKQ+KK3rms5UfV+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "vnc6Kf4zcT7O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "def word_tokenizer(input : str):\n",
        "  '''\n",
        "  Converts input string into an array of word tokens, splitting at any whitespace.\n",
        "  Parameters:\n",
        "    input (str): input with whitespace\n",
        "  Returns:\n",
        "    tokens (list of str)\n",
        "  '''\n",
        "  return input.split()\n",
        "class MarkovChain:\n",
        "  '''\n",
        "  MarkovChain - can be trained to get transition probabilities, can be evaluated to get next n words.\n",
        "  The format is kind of silly for markov chains, but should be something which can be\n",
        "  relatively unmodified as the model complexity and training sample size grow\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    return\n",
        "  def train(self, X : np.ndarray, pseudocount : int=0):\n",
        "    '''\n",
        "    Trains markov chain (populates transition probability matrix)\n",
        "    Parameters:\n",
        "      X (np.ndarray): tokens. This should already be of a reasonable vocabulary size (i.e. you should replace extra words with <unk> in the tokenizer!)\n",
        "      pseudocount (int): amount to add to each element of count matrix (as a form of regularization) \n",
        "    '''\n",
        "    unique = np.unique(X)\n",
        "    # I honestly am not sure if there's a way to do this which relies more on built-in numpy functions\n",
        "    # but its big-O runtime is constrained by O(v^2 + n) anyways, which I don't think we can improve upon.\n",
        "    token_to_idx = dict()\n",
        "    for i in range(len(unique)):\n",
        "      word = unique[i]\n",
        "      token_to_idx[word] = i\n",
        "    print(token_to_idx)\n",
        "    # we could use a sparse matrix here but that only works if we have 0 for pseudocount; probably fine either way.\n",
        "    # It's a toy model!\n",
        "    transition_matrix = np.zeros([len(unique), len(unique)])\n",
        "    prev_token = None\n",
        "    for token in X:\n",
        "      if prev_token is not None:\n",
        "        prev_idx = token_to_idx[prev_token]\n",
        "        cur_idx = token_to_idx[token]\n",
        "        transition_matrix[prev_idx, cur_idx] += 1\n",
        "      prev_token = token\n",
        "    for i in range(len(unique)):\n",
        "      transition_matrix[i] /= np.sum(transition_matrix[i])\n",
        "    print(transition_matrix)\n",
        "    self.transition_matrix = transition_matrix\n",
        "    self.token_to_idx = token_to_idx\n",
        "  def predict(self, X : np.ndarray, n : int=1):\n",
        "    '''\n",
        "    Continues input token array for next n tokens (cannot specify empty X)\n",
        "    Parameters:\n",
        "      X (np.ndarray): tokens to continue\n",
        "      n (int): number of tokens to output\n",
        "    Returns:\n",
        "      continuation (np.ndarray): tokens to output\n",
        "    '''\n",
        "    idx_to_token = {idx: token for token, idx in self.token_to_idx.items()}\n",
        "    token_to_idx = self.token_to_idx\n",
        "    prev_idx = token_to_idx[X[-1]]\n",
        "    res_tokens = []\n",
        "    for i in range(n):\n",
        "      transition_probs = self.transition_matrix[prev_idx]\n",
        "      next_idx = np.random.choice(len(token_to_idx), size=1, p=transition_probs)[0]\n",
        "      res_tokens.append(idx_to_token[next_idx])\n",
        "      prev_idx = next_idx\n",
        "    return np.array(res_tokens)\n",
        "  # TODO maybe worth adding a get_log_likelihood thing here to compare loss with other models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testStr = 'a a a b b b a a b b b b b b a a a a b b b c c a c c c a b b b c a a a c'\n",
        "tokens = word_tokenizer(testStr)\n",
        "model = MarkovChain()\n",
        "model.train(tokens)\n",
        "model.predict(tokens, 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0BQDvbvdNbj",
        "outputId": "73af06d9-ed59-4f5f-c2f3-3ca1cd7ae1e7"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': 0, 'b': 1, 'c': 2}\n",
            "[[0.57142857 0.28571429 0.14285714]\n",
            " [0.13333333 0.73333333 0.13333333]\n",
            " [0.5        0.         0.5       ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['c', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'c', 'a', 'a', 'a',\n",
              "       'b', 'b', 'a', 'a', 'c', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b',\n",
              "       'b', 'c', 'c', 'c', 'a', 'a', 'b', 'a', 'b', 'b', 'b', 'c', 'c',\n",
              "       'a', 'c', 'c', 'c', 'c', 'c', 'a', 'b', 'b', 'b', 'c'], dtype='<U1')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    }
  ]
}